#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ğŸš€ ULTRA FILE SEARCH BOT - ÙŠØ¯Ø¹Ù… Ø­ØªÙ‰ 2GB - Ø£Ø³Ø±Ø¹ ÙˆØ£Ù‚ÙˆÙ‰

import os
import sys
import json
import csv
import sqlite3
import pandas as pd
import numpy as np
import re
import zipfile
import hashlib
import chardet
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Generator

# Telegram
from telegram import (
    Update, 
    InlineKeyboardButton, 
    InlineKeyboardMarkup,
    InputFile,
    InputMediaDocument,
    ReplyKeyboardMarkup,
    KeyboardButton
)
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    CallbackQueryHandler,
    ContextTypes,
    ConversationHandler,
    PicklePersistence
)

# ==================== CONFIGURATION ====================
class Config:
    # Telegram Bot Token
    BOT_TOKEN = "7611903521:AAFv1xiXkFlJMErbpk7aTpKMS79bcnPPNSU"  # âœ… ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
    
    # Admin IDs
    ADMIN_IDS = [8493388920]
    
    # File size limits (2GB = 2 * 1024 * 1024 * 1024)
    MAX_FILE_SIZE = 2 * 1024 * 1024 * 1024  # 2GB
    MAX_MEMORY_USAGE = 1 * 1024 * 1024 * 1024  # 1GB RAM
    CHUNK_SIZE = 10 * 1024 * 1024  # 10MB chunks
    
    # Database
    DB_PATH = "ultra_file_search.db"
    CACHE_DIR = Path("cache")
    TEMP_DIR = Path("temp_files")
    LOG_DIR = Path("logs")
    
    # Performance
    MAX_WORKERS = 4  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ù„ØªØ¬Ù†Ø¨ multiprocessing
    CACHE_TTL = 3600  # 1 hour
    BATCH_SIZE = 1000
    TIMEOUT_SECONDS = 300  # 5 minutes
    
    # Security
    ALLOWED_EXTENSIONS = {
        '.csv', '.json', '.txt', '.xlsx', '.xls', '.xlsm',
        '.db', '.sqlite', '.sqlite3', '.parquet', '.feather',
        '.tsv', '.xml', '.yaml', '.yml'
    }
    
    MAX_FILES_PER_USER = 10
    MAX_SEARCHES_PER_DAY = 100
    
    @classmethod
    def setup(cls):
        """Create necessary directories"""
        for directory in [cls.CACHE_DIR, cls.TEMP_DIR, cls.LOG_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# ==================== ENHANCED LOGGING ====================
import logging

class EnhancedLogger:
    @staticmethod
    def setup():
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Console handler
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        console_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console.setFormatter(console_format)
        
        logger.addHandler(console)
        
        return logger

logger = EnhancedLogger.setup()

# ==================== SIMPLE FILE MANAGER ====================
class SimpleFileManager:
    """Ù…Ø¯ÙŠØ± Ù…Ù„ÙØ§Øª Ù…Ø¨Ø³Ø· Ø¨Ø¯ÙˆÙ† Ù…ÙƒØªØ¨Ø§Øª Ù…Ø­Ø¸ÙˆØ±Ø©"""
    
    def __init__(self):
        self.active_files = {}
        self.file_cache = {}
    
    def detect_encoding(self, file_path: str) -> str:
        """ÙƒØ´Ù ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù"""
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)
                
                # Simple encoding detection
                try:
                    raw_data.decode('utf-8')
                    return 'utf-8'
                except:
                    try:
                        raw_data.decode('utf-8-sig')
                        return 'utf-8-sig'
                    except:
                        try:
                            raw_data.decode('cp1256')
                            return 'cp1256'
                        except:
                            return 'latin-1'
        except Exception as e:
            logger.error(f"Encoding detection error: {e}")
            return 'utf-8'
    
    def get_file_size(self, file_path: str) -> int:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù"""
        try:
            return os.path.getsize(file_path)
        except:
            return 0
    
    def load_csv(self, file_path: str) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV"""
        try:
            encoding = self.detect_encoding(file_path)
            file_size = self.get_file_size(file_path)
            
            # Load based on size
            if file_size < 50 * 1024 * 1024:  # Ø£Ù‚Ù„ Ù…Ù† 50MB
                df = pd.read_csv(file_path, encoding=encoding, low_memory=False)
                loaded_fully = True
            else:
                # ØªØ­Ù…ÙŠÙ„ Ø£ÙˆÙ„ 10000 Ø³Ø·Ø± Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
                df = pd.read_csv(file_path, encoding=encoding, nrows=10000, low_memory=False)
                loaded_fully = False
            
            return {
                'type': 'csv',
                'data': df,
                'size': file_size,
                'rows': len(df),
                'columns': list(df.columns),
                'loaded_fully': loaded_fully
            }
                
        except Exception as e:
            logger.error(f"CSV loading error: {e}")
            return {'error': str(e)}
    
    def load_json(self, file_path: str) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON"""
        try:
            file_size = self.get_file_size(file_path)
            
            if file_size < 10 * 1024 * 1024:  # Ø£Ù‚Ù„ Ù…Ù† 10MB
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    df = pd.DataFrame(data[:10000])  # Limit to 10k records
                elif isinstance(data, dict):
                    df = pd.DataFrame([data])
                else:
                    df = pd.DataFrame({'data': [str(data)]})
                
                return {
                    'type': 'json',
                    'data': df,
                    'size': file_size,
                    'rows': len(df),
                    'columns': list(df.columns)
                }
            else:
                # For large JSON, read line by line
                data = []
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= 10000:  # Limit to 10k lines
                            break
                        try:
                            item = json.loads(line.strip())
                            data.append(item)
                        except:
                            continue
                
                df = pd.DataFrame(data) if data else pd.DataFrame()
                
                return {
                    'type': 'json',
                    'data': df,
                    'size': file_size,
                    'rows': len(df),
                    'columns': list(df.columns) if not df.empty else [],
                    'sample_only': True
                }
                
        except Exception as e:
            logger.error(f"JSON loading error: {e}")
            return {'error': str(e)}
    
    def load_excel(self, file_path: str) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Excel"""
        try:
            # Read first sheet
            df = pd.read_excel(file_path, nrows=10000)  # Limit to 10k rows
            
            return {
                'type': 'excel',
                'data': df,
                'size': self.get_file_size(file_path),
                'rows': len(df),
                'columns': list(df.columns)
            }
            
        except Exception as e:
            logger.error(f"Excel loading error: {e}")
            return {'error': str(e)}
    
    def load_sqlite(self, file_path: str) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite"""
        try:
            conn = sqlite3.connect(file_path)
            cursor = conn.cursor()
            
            # Get all tables
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [row[0] for row in cursor.fetchall()]
            
            if not tables:
                conn.close()
                return {'error': 'No tables found'}
            
            # Get first table data
            table = tables[0]
            df = pd.read_sql_query(f"SELECT * FROM {table} LIMIT 10000", conn)
            
            conn.close()
            
            return {
                'type': 'sqlite',
                'data': df,
                'size': self.get_file_size(file_path),
                'rows': len(df),
                'columns': list(df.columns),
                'table': table
            }
            
        except Exception as e:
            logger.error(f"SQLite loading error: {e}")
            return {'error': str(e)}
    
    def load_text(self, file_path: str) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù†ØµÙŠ"""
        try:
            encoding = self.detect_encoding(file_path)
            file_size = self.get_file_size(file_path)
            
            data = []
            with open(file_path, 'r', encoding=encoding) as f:
                for i, line in enumerate(f):
                    if i >= 10000:  # Limit to 10k lines
                        break
                    if line.strip():
                        data.append({
                            'line': i + 1,
                            'text': line.strip()[:500]  # Limit line length
                        })
            
            df = pd.DataFrame(data)
            
            return {
                'type': 'text',
                'data': df,
                'size': file_size,
                'rows': len(df)
            }
                
        except Exception as e:
            logger.error(f"Text loading error: {e}")
            return {'error': str(e)}
    
    def search_in_data(self, data: Dict, query: str) -> Dict:
        """Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        results = {
            'exact_matches': [],
            'partial_matches': [],
            'column_stats': {},
            'search_time': None
        }
        
        start_time = time.time()
        
        try:
            df = data.get('data', pd.DataFrame())
            if df.empty:
                return results
            
            query_lower = query.lower()
            
            # Search in each text column
            for column in df.columns:
                if df[column].dtype == 'object':
                    col_results = {
                        'column': column,
                        'exact_rows': [],
                        'partial_rows': []
                    }
                    
                    # Exact match
                    exact_mask = df[column].astype(str).str.lower() == query_lower
                    if exact_mask.any():
                        exact_rows = df[exact_mask].head(10)
                        for _, row in exact_rows.iterrows():
                            col_results['exact_rows'].append(row.to_dict())
                    
                    # Partial match
                    partial_mask = df[column].astype(str).str.lower().str.contains(query_lower, na=False)
                    if partial_mask.any() and not exact_mask.all():
                        partial_rows = df[partial_mask & ~exact_mask].head(10)
                        for _, row in partial_rows.iterrows():
                            col_results['partial_rows'].append(row.to_dict())
                    
                    # Add to results
                    if col_results['exact_rows'] or col_results['partial_rows']:
                        results['exact_matches'].extend(col_results['exact_rows'])
                        results['partial_matches'].extend(col_results['partial_rows'])
                        
                        results['column_stats'][column] = {
                            'exact': len(col_results['exact_rows']),
                            'partial': len(col_results['partial_rows'])
                        }
            
            results['search_time'] = time.time() - start_time
            
            return results
            
        except Exception as e:
            logger.error(f"Search error: {e}")
            results['error'] = str(e)
            return results
    
    def simple_search(self, file_path: str, query: str, file_type: str) -> Dict:
        """Ø¨Ø­Ø« Ù…Ø¨Ø³Ø·"""
        try:
            # Load file based on type
            if file_type == 'csv':
                file_data = self.load_csv(file_path)
            elif file_type == 'json':
                file_data = self.load_json(file_path)
            elif file_type in ['xlsx', 'xls', 'xlsm']:
                file_data = self.load_excel(file_path)
            elif file_type in ['db', 'sqlite', 'sqlite3']:
                file_data = self.load_sqlite(file_path)
            else:
                file_data = self.load_text(file_path)
            
            if 'error' in file_data:
                return {'error': file_data['error']}
            
            # Search in loaded data
            return self.search_in_data(file_data, query)
            
        except Exception as e:
            logger.error(f"Simple search error: {e}")
            return {'error': str(e)}

# ==================== SIMPLE DATABASE ====================
class SimpleDatabase:
    """Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø³Ø·Ø©"""
    
    def __init__(self):
        self.db_path = Config.DB_PATH
        self.init_database()
    
    def init_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Users table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                user_id INTEGER PRIMARY KEY,
                username TEXT,
                first_name TEXT,
                last_name TEXT,
                join_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                total_files INTEGER DEFAULT 0,
                total_searches INTEGER DEFAULT 0
            )
        ''')
        
        # Files table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS files (
                file_id TEXT PRIMARY KEY,
                user_id INTEGER,
                original_name TEXT,
                file_size INTEGER,
                file_type TEXT,
                upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_hash TEXT
            )
        ''')
        
        # Searches table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS searches (
                search_id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                query TEXT,
                results_count INTEGER,
                search_time INTEGER,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def log_user_activity(self, user_id: int, username: str, first_name: str):
        """ØªØ³Ø¬ÙŠÙ„ Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO users 
                (user_id, username, first_name, last_active) 
                VALUES (?, ?, ?, ?)
            ''', (user_id, username or '', first_name or '', datetime.now()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"User activity logging error: {e}")
    
    def save_file_info(self, file_info: Dict):
        """Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO files 
                (file_id, user_id, original_name, file_size, file_type, upload_date, file_hash)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                file_info.get('file_id'),
                file_info.get('user_id'),
                file_info.get('original_name'),
                file_info.get('file_size'),
                file_info.get('file_type'),
                datetime.now(),
                file_info.get('file_hash')
            ))
            
            # Update user stats
            cursor.execute('''
                UPDATE users 
                SET total_files = total_files + 1, 
                    last_active = ?
                WHERE user_id = ?
            ''', (datetime.now(), file_info.get('user_id')))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"File info saving error: {e}")
    
    def log_search(self, search_info: Dict):
        """ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø«"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO searches 
                (user_id, query, results_count, search_time, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                search_info.get('user_id'),
                search_info.get('query'),
                search_info.get('results_count', 0),
                search_info.get('search_time', 0),
                datetime.now()
            ))
            
            # Update user stats
            cursor.execute('''
                UPDATE users 
                SET total_searches = total_searches + 1,
                    last_active = ?
                WHERE user_id = ?
            ''', (datetime.now(), search_info.get('user_id')))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Search logging error: {e}")
    
    def get_user_stats(self, user_id: int) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT total_files, total_searches, join_date, last_active
                FROM users WHERE user_id = ?
            ''', (user_id,))
            
            row = cursor.fetchone()
            conn.close()
            
            if row:
                return {
                    'total_files': row[0],
                    'total_searches': row[1],
                    'join_date': row[2],
                    'last_active': row[3]
                }
            return {}
            
        except Exception as e:
            logger.error(f"Get user stats error: {e}")
            return {}
    
    def check_rate_limit(self, user_id: int) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Count today's searches
            today = datetime.now().date()
            cursor.execute('''
                SELECT COUNT(*) FROM searches 
                WHERE user_id = ? AND DATE(timestamp) = ?
            ''', (user_id, today))
            
            count = cursor.fetchone()[0]
            conn.close()
            
            return count < Config.MAX_SEARCHES_PER_DAY
            
        except Exception as e:
            logger.error(f"Rate limit check error: {e}")
            return True

# ==================== POWERFUL TELEGRAM BOT ====================
class UltraFileSearchBot:
    def __init__(self, token: str):
        self.token = token
        self.file_manager = SimpleFileManager()
        self.database = SimpleDatabase()
        self.user_sessions = {}
        self.app = None
    
    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª"""
        user = update.effective_user
        
        # Log user activity
        self.database.log_user_activity(
            user.id, 
            user.username, 
            user.first_name
        )
        
        welcome = f"""
ğŸš€ **Ø£Ù‡Ù„Ø§Ù‹ {user.first_name}!**

âš¡ **Ultra File Search Bot v2.0**
Ø¨ÙˆØª Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù‚ÙˆÙ‰!

ğŸ“Š **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:**
âœ… Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª Ø­ØªÙ‰ 2GB
âš¡ Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„
ğŸ” Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©
ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

ğŸ“ **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
â€¢ CSV, JSON, TXT
â€¢ Excel (XLSX, XLS)
â€¢ SQLite Databases
â€¢ XML, YAML

âš¡ **ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
1. Ø£Ø±Ø³Ù„ Ù„ÙŠ Ù…Ù„ÙØ§Ù‹
2. Ø§Ù†ØªØ¸Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„
3. Ø§ÙƒØªØ¨ Ù…Ø§ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡
4. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

ğŸ“Š **Ø¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙƒ:** /stats
ğŸ†˜ **Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©:** /help

ğŸš€ **Ø£Ø±Ø³Ù„ Ù…Ù„ÙØ§Ù‹ Ø§Ù„Ø¢Ù† Ù„ØªØ¨Ø¯Ø£!**
        """
        
        keyboard = [
            [KeyboardButton("ğŸ“ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù")],
            [KeyboardButton("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙŠ"), KeyboardButton("ğŸ†˜ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©")]
        ]
        
        reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True)
        
        await update.message.reply_text(
            welcome,
            parse_mode='Markdown',
            reply_markup=reply_markup
        )
    
    async def handle_document(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø©"""
        user = update.effective_user
        document = update.message.document
        
        if not document:
            await update.message.reply_text("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù ØµØ§Ù„Ø­")
            return
        
        # Check file size
        file_size = document.file_size
        if file_size > Config.MAX_FILE_SIZE:
            await update.message.reply_text(
                f"âŒ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ({file_size / 1024 / 1024:.2f}MB) ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹"
            )
            return
        
        # Check file extension
        file_name = document.file_name
        file_ext = os.path.splitext(file_name)[1].lower()
        
        if file_ext not in Config.ALLOWED_EXTENSIONS:
            await update.message.reply_text(
                f"âŒ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file_ext}"
            )
            return
        
        # Send processing message
        processing_msg = await update.message.reply_text(
            f"ğŸ”„ **Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù:** `{file_name}`",
            parse_mode='Markdown'
        )
        
        try:
            # Download file
            file_dir = Config.TEMP_DIR / str(user.id)
            file_dir.mkdir(parents=True, exist_ok=True)
            
            file_path = file_dir / f"{int(time.time())}_{file_name}"
            file = await context.bot.get_file(document.file_id)
            await file.download_to_drive(file_path)
            
            # Generate file hash
            file_hash = self.calculate_file_hash(file_path)
            
            # Process file
            start_time = time.time()
            
            # Load file based on type
            if file_ext == '.csv':
                file_data = self.file_manager.load_csv(file_path)
            elif file_ext == '.json':
                file_data = self.file_manager.load_json(file_path)
            elif file_ext in ['.xlsx', '.xls', '.xlsm']:
                file_data = self.file_manager.load_excel(file_path)
            elif file_ext in ['.db', '.sqlite', '.sqlite3']:
                file_data = self.file_manager.load_sqlite(file_path)
            else:
                file_data = self.file_manager.load_text(file_path)
            
            processing_time = time.time() - start_time
            
            if 'error' in file_data:
                await processing_msg.edit_text(
                    f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù:**\n`{file_data['error']}`"
                )
                os.remove(file_path)
                return
            
            # Save to database
            self.database.save_file_info({
                'file_id': file_hash,
                'user_id': user.id,
                'original_name': file_name,
                'file_size': file_size,
                'file_type': file_ext.replace('.', ''),
                'file_hash': file_hash
            })
            
            # Prepare response
            rows_info = file_data['rows']
            columns = file_data.get('columns', [])
            
            await processing_msg.edit_text(
                f"âœ… **ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­!**\n\n"
                f"ğŸ“Š **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù:**\n"
                f"â€¢ **Ø§Ù„ØµÙÙˆÙ:** {rows_info:,}\n"
                f"â€¢ **Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:** {len(columns)}\n"
                f"â€¢ **Ø§Ù„Ù†ÙˆØ¹:** {file_ext.upper()}\n\n"
                f"ğŸ” **Ø§ÙƒØªØ¨ Ø§Ù„Ø¢Ù† Ù…Ø§ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡:**",
                parse_mode='Markdown'
            )
            
            # Store session
            self.user_sessions[user.id] = {
                'file_path': str(file_path),
                'file_hash': file_hash,
                'file_name': file_name,
                'file_data': file_data,
                'file_size': file_size,
                'file_type': file_ext.replace('.', ''),
                'last_active': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"File processing error: {e}")
            await processing_msg.edit_text(
                f"âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£:**\n`{str(e)[:100]}`"
            )
    
    async def handle_text(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ø¨Ø­Ø«"""
        user = update.effective_user
        query = update.message.text.strip()
        
        if not query:
            await update.message.reply_text("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„Ø¨Ø­Ø«")
            return
        
        # Check if user has active file
        if user.id not in self.user_sessions:
            await update.message.reply_text(
                "âŒ Ù„Ù… ØªÙ‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø¨Ø¹Ø¯\n"
                "ğŸ“ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ø£ÙˆÙ„Ø§Ù‹"
            )
            return
        
        # Check rate limit
        if not self.database.check_rate_limit(user.id):
            await update.message.reply_text(
                "â° **Ù„Ù‚Ø¯ ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø¨Ø­Ø«**"
            )
            return
        
        # Get session info
        session = self.user_sessions[user.id]
        file_path = session['file_path']
        file_data = session['file_data']
        
        # Send searching message
        search_msg = await update.message.reply_text(
            f"ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†:** `{query}`",
            parse_mode='Markdown'
        )
        
        try:
            start_time = time.time()
            
            # Perform search
            results = self.file_manager.search_in_data(file_data, query)
            search_time = time.time() - start_time
            
            if 'error' in results:
                await search_msg.edit_text(
                    f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«:**\n`{results['error']}`"
                )
                return
            
            # Calculate total matches
            total_matches = len(results.get('exact_matches', [])) + len(results.get('partial_matches', []))
            
            # Log search
            self.database.log_search({
                'user_id': user.id,
                'query': query,
                'results_count': total_matches,
                'search_time': search_time
            })
            
            # Create report
            report = self.create_search_report(
                query, 
                results, 
                session, 
                search_time,
                total_matches
            )
            
            # Send results
            await search_msg.edit_text(
                report,
                parse_mode='Markdown'
            )
            
            # Send CSV if results found
            if total_matches > 0:
                await self.send_results_csv(update, results, session['file_name'])
            
        except Exception as e:
            logger.error(f"Search error: {e}")
            await search_msg.edit_text(
                f"âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£:**\n`{str(e)[:100]}`"
            )
    
    def create_search_report(self, query: str, results: Dict, session: Dict, 
                           search_time: float, total_matches: int) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø¹Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«"""
        report = f"""
ğŸ“Š **Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†:** `{query}`

âœ… **Ø§Ù„Ù…Ù„Ø®Øµ:**
â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: **{total_matches:,}**
â€¢ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø­Ø«: **{search_time:.2f} Ø«Ø§Ù†ÙŠØ©**
â€¢ Ø§Ù„Ù…Ù„Ù: **{session['file_name']}**
â€¢ Ø§Ù„Ø­Ø¬Ù…: **{session['file_size'] / 1024 / 1024:.2f} MB**
"""
        
        # Add match types
        exact_count = len(results.get('exact_matches', []))
        partial_count = len(results.get('partial_matches', []))
        
        if exact_count > 0:
            report += f"\nâœ… **Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ§Ù…Ø©:** {exact_count:,}"
        
        if partial_count > 0:
            report += f"\nğŸ” **Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©:** {partial_count:,}"
        
        # Add column statistics
        if results.get('column_stats'):
            report += "\n\nğŸ“ˆ **Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:**"
            for column, stats in list(results['column_stats'].items())[:3]:
                report += f"\nâ€¢ `{column}`: {stats.get('exact', 0)} ØªØ§Ù…ØŒ {stats.get('partial', 0)} Ø¬Ø²Ø¦ÙŠ"
        
        # Add sample results
        sample_results = []
        if results.get('exact_matches'):
            sample_results.extend(results['exact_matches'][:2])
        elif results.get('partial_matches'):
            sample_results.extend(results['partial_matches'][:2])
        
        if sample_results:
            report += "\n\nğŸ” **Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬:**"
            for i, result in enumerate(sample_results[:2], 1):
                if isinstance(result, dict):
                    items = list(result.items())[:2]
                    result_text = "\n".join([f"  â€¢ {k}: {v}" for k, v in items])
                    report += f"\n\n**Ø§Ù„Ù†ØªÙŠØ¬Ø© {i}:**\n{result_text}"
        
        # Add recommendations
        if total_matches == 0:
            report += "\n\nğŸ’¡ **Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬**\n"
            report += "â€¢ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©\n"
            report += "â€¢ ØªØ£ÙƒØ¯ Ù…Ù† ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­"
        
        report += f"\n\nâ° **Ø§Ù„ÙˆÙ‚Øª:** {datetime.now().strftime('%H:%M:%S')}"
        
        return report
    
    async def send_results_csv(self, update: Update, results: Dict, file_name: str):
        """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ…Ù„Ù CSV"""
        try:
            # Combine all matches
            all_matches = []
            
            if results.get('exact_matches'):
                all_matches.extend(results['exact_matches'])
            
            if results.get('partial_matches'):
                all_matches.extend(results['partial_matches'])
            
            if not all_matches:
                return
            
            # Create CSV
            csv_data = []
            
            for i, match in enumerate(all_matches[:500], 1):  # Limit to 500 results
                if isinstance(match, dict):
                    row = {'result_number': i, **match}
                    csv_data.append(row)
            
            if csv_data:
                # Create DataFrame and save as CSV
                df = pd.DataFrame(csv_data)
                csv_path = Config.TEMP_DIR / f"results_{int(time.time())}.csv"
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                
                # Send file
                with open(csv_path, 'rb') as f:
                    await update.message.reply_document(
                        document=f,
                        filename=f"Ù†ØªØ§Ø¦Ø¬_{file_name}.csv",
                        caption="ğŸ“¥ **ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬**"
                    )
                
                # Clean up
                os.remove(csv_path)
                
        except Exception as e:
            logger.error(f"CSV export error: {e}")
    
    async def stats_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        user = update.effective_user
        
        stats = self.database.get_user_stats(user.id)
        
        if stats:
            stats_msg = f"""
ğŸ“Š **Ø¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙƒ Ø§Ù„Ø´Ø®ØµÙŠØ©**

ğŸ‘¤ **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:**
â€¢ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©: **{stats['total_files']}**
â€¢ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø«: **{stats['total_searches']}**
â€¢ Ø¢Ø®Ø± Ù†Ø´Ø§Ø·: **{stats['last_active']}**

ğŸ’¡ **Ù†ØµØ§Ø¦Ø­:**
â€¢ Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø¨Ø­Ø«: {Config.MAX_SEARCHES_PER_DAY} Ø¹Ù…Ù„ÙŠØ©
â€¢ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ù„Ù Ø¹Ø¯Ø© Ù…Ø±Ø§Øª
            """
        else:
            stats_msg = "ğŸ“Š **Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ø£ÙˆÙ„Ø§Ù‹!**"
        
        await update.message.reply_text(stats_msg, parse_mode='Markdown')
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø£Ù…Ø± Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"""
        help_text = """
ğŸ†˜ **Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…**

ğŸš€ **ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
1. Ø£Ø±Ø³Ù„ `/start` Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
2. Ø£Ø±Ø³Ù„ Ù…Ù„ÙØ§Ù‹ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡
3. Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
4. Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡
5. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

ğŸ“ **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
â€¢ CSV, JSON, TXT
â€¢ Excel (XLSX, XLS)
â€¢ SQLite Databases
â€¢ XML, YAML

ğŸ” **Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨Ø­Ø«:**
âœ… **ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù…:** Ù†ÙØ³ Ø§Ù„Ù†Øµ ØªÙ…Ø§Ù…Ø§Ù‹
ğŸ” **ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ:** ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ

ğŸ“Š **Ø§Ù„Ø£ÙˆØ§Ù…Ø±:**
/start - Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
/stats - Ø¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙƒ
/help - Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©

âš ï¸ **Ù…Ù„Ø§Ø­Ø¸Ø§Øª:**
â€¢ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ù„Ù: 2GB
â€¢ Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ: 100 Ø¹Ù…Ù„ÙŠØ© Ø¨Ø­Ø«
â€¢ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ­ÙØ¸ ÙƒÙ…Ù„Ù CSV
        """
        
        await update.message.reply_text(help_text, parse_mode='Markdown')
    
    async def clear_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ø³Ø­ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        user = update.effective_user
        
        try:
            # Clear user session
            if user.id in self.user_sessions:
                del self.user_sessions[user.id]
            
            # Clear user temp files
            user_dir = Config.TEMP_DIR / str(user.id)
            if user_dir.exists():
                import shutil
                shutil.rmtree(user_dir)
            
            await update.message.reply_text(
                "âœ… **ØªÙ… Ø§Ù„Ù…Ø³Ø­ Ø¨Ù†Ø¬Ø§Ø­**\n"
                "ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯"
            )
            
        except Exception as e:
            logger.error(f"Clear error: {e}")
            await update.message.reply_text("âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³Ø­**")
    
    def calculate_file_hash(self, file_path: str) -> str:
        """Ø­Ø³Ø§Ø¨ Ø¨ØµÙ…Ø© Ø§Ù„Ù…Ù„Ù"""
        try:
            hasher = hashlib.sha256()
            
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    hasher.update(chunk)
            
            return hasher.hexdigest()
            
        except Exception as e:
            logger.error(f"Hash error: {e}")
            return str(int(time.time()))
    
    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª"""
        self.app = Application.builder().token(self.token).build()
        
        # Add handlers
        self.app.add_handler(CommandHandler("start", self.start))
        self.app.add_handler(CommandHandler("stats", self.stats_command))
        self.app.add_handler(CommandHandler("help", self.help_command))
        self.app.add_handler(CommandHandler("clear", self.clear_command))
        
        self.app.add_handler(MessageHandler(filters.Document.ALL, self.handle_document))
        self.app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_text))
        
        # Run bot
        print("\n" + "="*60)
        print("ğŸš€ ULTRA FILE SEARCH BOT v2.0")
        print("="*60)
        print(f"âœ… Bot Token: {self.token[:10]}...")
        print(f"ğŸ‘‘ Admin ID: {Config.ADMIN_IDS[0]}")
        print(f"ğŸ“ Max File Size: {Config.MAX_FILE_SIZE / 1024 / 1024 / 1024:.1f} GB")
        print("="*60)
        print("âœ… Bot is running...")
        print("="*60)
        
        self.app.run_polling(allowed_updates=Update.ALL_UPDATES)

# ==================== MAIN ====================
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    # Setup configuration
    Config.setup()
    
    try:
        # Create and run bot
        bot = UltraFileSearchBot(Config.BOT_TOKEN)
        bot.run()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()